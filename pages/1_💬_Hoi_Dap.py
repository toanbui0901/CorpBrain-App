import streamlit as st
import sys
import os

# Fix Ä‘Æ°á»ng dáº«n import
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils import get_llm, DB_DIR

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

st.set_page_config(page_title="Há»i Ä‘Ã¡p", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Há»i Ä‘Ã¡p Quy Ä‘á»‹nh Ná»™i bá»™")

with st.sidebar:
    st.header("âš™ï¸ Cáº¥u hÃ¬nh")
    model_choice = st.selectbox("Model", ["Gemini 2.5 Flash", "DeepSeek R1 (OpenRouter)"])
    api_key = st.text_input("API Key", type="password")
    
    # TÃ¹y chá»‰nh Ä‘á»™ sÃ¢u tÃ¬m kiáº¿m
    search_k = st.slider("Äá»™ sÃ¢u tÃ¬m kiáº¿m (Sá»‘ Ä‘oáº¡n vÄƒn)", min_value=3, max_value=20, value=10)

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ChÃ o báº¡n, tÃ´i Ä‘Ã£ sáºµn sÃ ng tra cá»©u thÃ´ng tin cho báº¡n."}]

for m in st.session_state.messages:
    st.chat_message(m["role"]).markdown(m["content"])

if prompt := st.chat_input("Nháº­p cÃ¢u há»i..."):
    if not api_key:
        st.error("âš ï¸ ChÆ°a nháº­p API Key!")
        st.stop()
        
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” Äang quÃ©t cÆ¡ sá»Ÿ dá»¯ liá»‡u & Tá»•ng há»£p..."):
            try:
                if not os.path.exists(DB_DIR):
                    st.error("ChÆ°a cÃ³ dá»¯ liá»‡u. Vui lÃ²ng náº¡p file á»Ÿ trang Quáº£n lÃ½.")
                    st.stop()
                    
                emb_func = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                vector_db = Chroma(persist_directory=DB_DIR, embedding_function=emb_func)
                
                # TÄƒng sá»‘ lÆ°á»£ng Ä‘oáº¡n vÄƒn tÃ¬m kiáº¿m
                retriever = vector_db.as_retriever(search_kwargs={"k": search_k})
                
                llm = get_llm(model_choice, api_key)
                
                sys_prompt = (
                    "Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n quy Ä‘á»‹nh ná»™i bá»™ doanh nghiá»‡p. "
                    "Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn Context dÆ°á»›i Ä‘Ã¢y.\n"
                    "YÃŠU Cáº¦U: Tráº£ lá»i chi tiáº¿t, dÃ¹ng gáº¡ch Ä‘áº§u dÃ²ng, trÃ­ch dáº«n Metadata nguá»“n.\n"
                    "Dá»¯ liá»‡u tra cá»©u:\n{context}"
                )
                
                prompt_template = ChatPromptTemplate.from_messages([
                    ("system", sys_prompt),
                    ("human", "{input}")
                ])
                
                chain = create_retrieval_chain(
                    retriever, 
                    create_stuff_documents_chain(llm, prompt_template)
                )
                
                res = chain.invoke({"input": prompt})
                ans = res['answer']
                
                # TrÃ­ch dáº«n nguá»“n
                sources = {}
                for doc in res['context']:
                    name = doc.metadata.get('doc_name', 'KhÃ´ng tÃªn')
                    dept = doc.metadata.get('department', 'N/A')
                    sources[f"{name} ({dept})"] = True
                
                if sources:
                    ans += "\n\n---\n**ğŸ“š TÃ i liá»‡u tham kháº£o:**\n" + "\n".join([f"- {s}" for s in sources.keys()])
                
                st.markdown(ans)
                st.session_state.messages.append({"role": "assistant", "content": ans})
                
            except Exception as e:
                st.error(f"Lá»—i xá»­ lÃ½: {e}")
