import os
import sys
import platform
import streamlit as st

# --- FIX L·ªñI SQLITE (B·∫ÆT BU·ªòC ·ªû ƒê·∫¶U) ---
if platform.system() != "Windows":
    try:
        __import__('pysqlite3')
        sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    except ImportError:
        pass
# ---------------------------------------

import tempfile
import pandas as pd
from datetime import datetime

try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
    from langchain_chroma import Chroma
    from langchain_core.documents import Document
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import Docx2txtLoader
    from pypdf import PdfReader
    from pdf2image import convert_from_path
    import pytesseract
except ImportError:
    st.error("Thi·∫øu th∆∞ vi·ªán! Ki·ªÉm tra requirements.txt")
    st.stop()

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
if platform.system() == "Windows":
    TESSERACT_PATH = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
    POPPLER_PATH = r"C:\Program Files\poppler-24.08.0\Library\bin"
    if os.path.exists(TESSERACT_PATH):
        pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH
else:
    TESSERACT_PATH = "tesseract"
    POPPLER_PATH = None

DB_DIR = "vector_db"
HISTORY_FILE = "file_history.csv"

# --- H√ÄM H·ªñ TR·ª¢ OCR ---
def extract_text_with_ocr(file_path):
    text = ""
    try:
        reader = PdfReader(file_path)
        for page in reader.pages:
            extracted = page.extract_text()
            if extracted: text += extracted + "\n"
    except: pass

    # N·∫øu √≠t ch·ªØ qu√° th√¨ coi l√† file scan
    if len(text) < 50:
        st.toast("üì∑ ƒêang OCR tr√™n Cloud...", icon="‚òÅÔ∏è")
        try:
            if platform.system() == "Windows":
                images = convert_from_path(file_path, dpi=200, poppler_path=POPPLER_PATH) # Gi·∫£m DPI xu·ªëng 200 cho nh·∫π RAM
            else:
                images = convert_from_path(file_path, dpi=200)
            
            ocr_text = ""
            for img in images:
                ocr_text += pytesseract.image_to_string(img, lang='vie+eng') + "\n"
            return ocr_text
        except Exception as e:
            return f"L·ªói OCR: {e}"
    return text

# --- H√ÄM X·ª¨ L√ù CH√çNH (S·ª¨ D·ª§NG GOOGLE EMBEDDING) ---
def process_and_save(uploaded_file, meta_info, api_key):
    """
    C·∫ßn truy·ªÅn th√™m api_key v√†o ƒë·ªÉ embedding
    """
    if not api_key:
        st.error("C·∫ßn nh·∫≠p API Key ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu!")
        return 0

    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp:
        tmp.write(uploaded_file.getbuffer())
        fpath = tmp.name

    # ƒê·ªçc file
    text = ""
    if uploaded_file.name.endswith('.pdf'):
        text = extract_text_with_ocr(fpath)
    elif uploaded_file.name.endswith('.docx'):
        loader = Docx2txtLoader(fpath)
        docs = loader.load()
        text = "\n".join([d.page_content for d in docs])
    elif uploaded_file.name.endswith('.xlsx'):
        try:
            df = pd.read_excel(fpath)
            text = df.to_string(index=False)
        except: pass
    
    os.remove(fpath)
    if not text.strip(): return 0

    full_content = (
        f"METADATA >> [T√™n: {meta_info['doc_name']}] | [ƒê∆°n v·ªã: {meta_info['department']}] | [Ng√†y HL: {meta_info['effective_date']}]\n"
        f"N·ªòI DUNG:\n{text}"
    )
    doc = Document(page_content=full_content, metadata=meta_info)
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents([doc])

    # [THAY ƒê·ªîI QUAN TR·ªåNG] D√πng Google Embedding thay v√¨ HuggingFace (Ti·∫øt ki·ªám 500MB RAM)
    try:
        emb_func = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
        
        # Reset DB n·∫øu l·ªói
        try:
            vector_db = Chroma(persist_directory=DB_DIR, embedding_function=emb_func)
        except:
            import shutil
            if os.path.exists(DB_DIR): shutil.rmtree(DB_DIR)
            vector_db = Chroma(persist_directory=DB_DIR, embedding_function=emb_func)

        vector_db.add_documents(splits)
    except Exception as e:
        st.error(f"L·ªói Embedding (Ki·ªÉm tra API Key): {e}")
        return 0
    
    # Ghi log
    log_entry = {
        "File g·ªëc": uploaded_file.name,
        "T√™n vƒÉn b·∫£n": meta_info['doc_name'],
        "Ng√†y n·∫°p": datetime.now().strftime("%Y-%m-%d %H:%M")
    }
    if os.path.exists(HISTORY_FILE):
        df_hist = pd.read_csv(HISTORY_FILE)
        df_hist = pd.concat([df_hist, pd.DataFrame([log_entry])], ignore_index=True)
    else:
        df_hist = pd.DataFrame([log_entry])
    df_hist.to_csv(HISTORY_FILE, index=False)
    
    return len(splits)

def get_llm(model_type, api_key):
    if model_type == "DeepSeek R1 (OpenRouter)":
        return ChatOpenAI(base_url="https://openrouter.ai/api/v1", api_key=api_key, model="deepseek/deepseek-r1:free", temperature=0.3)
    elif model_type == "Gemini 2.5 Flash":
        return ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=api_key, temperature=0.1)
    return None
